{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Text Classification with a Pre-Trained Language Model using .Net\n",
        "The purpose of this notebook is to show how to use pre-trained weights from BERT (or another Tensorflow) 'language' model to train a classifier in .Net (specifically F#).\n",
        "\n",
        "The text classification task is more easily accomplished in Python due to the supportive ecosystem available there. The website [Hugging Face](https://huggingface.co/transformers/) contains 1000's of pre-trained language models that can be easily consumed using tooling supplied by Hugging Face. \n",
        "\n",
        "Python however is not the language of choice when it comes to building high-performance applications. To consume language (or other deep learning) models from an application one usually resorts to deploying the model as a service - with attendant cost, security and integration concerns. For a high-performance application, there may be a need to more tightly integrate the model with other application functionality and therefore an embedded model may be required.\n",
        "\n",
        "This notebook shows how a language model maybe re-trained and used directly from .Net, bypassing the need to deploy the model as a service."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "dotnet_interactive": {
          "language": "fsharp"
        }
      },
      "source": [
        "//#r \"nuget: libtorch-cpu-win-x64\" //this notebook is written to work on CPU also\n",
        "#r \"nuget: libtorch-cuda-11.3-win-x64, 1.10.0.1\" //large package - takes a long time to load and unpack the first time.\n",
        "#r \"nuget: TorchSharp\"\n",
        "#r \"nuget: TfCheckpoint\"   \n",
        "#r \"nuget: FsBERTTokenizer\"\n",
        "#r \"nuget: FSharp.Data\""
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": "<div><div></div><div></div><div><strong>Installed Packages</strong><ul><li><span>FsBERTTokenizer, 1.0.0</span></li><li><span>FSharp.Data, 4.2.5</span></li><li><span>TfCheckpoint, 1.0.0</span></li><li><span>TorchSharp, 0.95.3</span></li></ul></div></div>"
          },
          "execution_count": 1,
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "dotnet_interactive": {
          "language": "fsharp"
        }
      },
      "source": [
        "open TfCheckpoint\n",
        "open TorchSharp\n",
        "\n",
        "let device = if torch.cuda.is_available() then torch.CUDA else torch.CPU\n",
        "printfn $\"torch devices is %A{device}\""
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.code.notebook.stdout": "torch devices is cuda\r\n"
          },
          "execution_count": 1,
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load weigths from pre-trained BERT 'checkpoint'\n",
        "Here the pre-trained weights from the 'small' BERT uncased model are used - downloaded from [Tensorflow Hub](https://tfhub.dev/google/small_bert/bert_uncased_L-2_H-128_A-2/2).\n",
        "\n",
        "Note: The weights can also be downloaded from Hugging Face, however they are not easily extractable from languages other than Python. Hugging Face creates its own wrapped packages that require Hugging Face tooling to use.\n",
        "\n",
        "The download includes a folder called 'variables' that contains the pre-trained weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "dotnet_interactive": {
          "language": "fsharp"
        }
      },
      "source": [
        "let bertCheckpointFolder = @\"C:\\s\\hack\\small_bert_bert_uncased_L-2_H-128_A-2_2\\variables\"\n",
        "let tensors = CheckpointReader.readCheckpoint bertCheckpointFolder |> Seq.toArray\n",
        "//show first tensor\n",
        "printfn \"%A\" tensors.[0]"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.code.notebook.stdout": "(\"bert/embeddings/LayerNorm/beta\",\n { Shape = [|128L|]\n   Tensor =\n    TdFloat\n      [|0.1427177936f; 0.1417384148f; 0.1129989177f; 0.008431605063f;\n        -0.3839171827f; -0.04579306394f; -0.009391464293f; 0.2562615871f;\n        0.02031775191f; -0.1169935018f; 0.04341379181f; -0.03693608567f;\n        -0.1498966217f; -0.04671567678f; -0.05263318121f; -0.1550539136f;\n        0.1118891314f; -0.2280282229f; 0.01878583431f; 0.0003494967241f;\n        0.1215098947f; -0.2769323587f; -0.2301771045f; -0.593540132f;\n        -0.01692879945f; -0.005055753049f; 0.09231737256f; 0.01337191183f;\n        -0.2198150158f; -0.3343744278f; -0.3485637009f; -0.4727560282f;\n        -0.03425497934f; 0.01004845928f; -0.009842976928f; -0.1552859247f;\n        -0.1210347489f; 0.09292084724f; 0.1551780701f; -0.2503150105f;\n        0.01316787116f; -0.06462553144f; -0.4156924486f; 0.1022200063f;\n        -0.03140406683f; 0.2214509994f; 0.09048749506f; 0.2421075106f;\n        -0.2365095466f; 0.06668421626f; 0.1963646859f; 0.1197723746f;\n        -0.08394248784f; 0.2961205542f; 0.2564467788f; 0.0543651022f;\n        0.1783883423f; -0.2799687386f; -0.145049125f; -0.1192677319f;\n        -0.07890660316f; -0.1987643242f; -0.3815532327f; 0.2429409772f;\n        -0.1923356205f; -0.08475973457f; -0.03282091022f; -0.03904989734f;\n        -0.05057211965f; 0.1082714349f; -0.0855319798f; 0.2224779278f;\n        -0.1498326063f; -0.3143232167f; -0.07393050939f; -0.02140595764f;\n        -0.1140864119f; 0.06725683063f; -0.3671147227f; -0.8035869002f;\n        -0.2137966454f; 0.2168199122f; -0.04170248285f; -0.5436524749f;\n        0.03476501256f; -0.2010415047f; -0.05343238264f; 0.07493341714f;\n        -0.07146064937f; 0.1322321892f; -0.09058468789f; -0.05177679658f;\n        -0.06401548535f; 0.2640584111f; -0.1561880112f; -0.07549760491f;\n        0.06026218832f; 0.0143991299f; 0.2228940427f; 0.2273250818f; ...|] })\r\n"
          },
          "execution_count": 1,
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the above output, the first tensor is named *\"bert/embeddings/LayerNorm/beta\"*. It is a float32 array of shape 1x128. \n",
        "\n",
        "Note the TfCheckpoint package keeps tensors as flat arrays. These can be reshaped when loading into other Tensor libraries e.g. TorchSharp as shown later.\n",
        "\n",
        "### List checkpoint tensors\n",
        "Below are all the tensor names in the pre-trained BERT checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "dotnet_interactive": {
          "language": "fsharp"
        }
      },
      "source": [
        "tensors (* |> Array.skip 20 *) |> Array.map (fun (n,st) -> {|Dims=st.Shape; Name=n|})"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": "<table><thead><tr><th><i>index</i></th><th>Dims</th><th>Name</th></tr></thead><tbody><tr><td>0</td><td><div class=\"dni-plaintext\">[ 128 ]</div></td><td>bert/embeddings/LayerNorm/beta</td></tr><tr><td>1</td><td><div class=\"dni-plaintext\">[ 128 ]</div></td><td>bert/embeddings/LayerNorm/gamma</td></tr><tr><td>2</td><td><div class=\"dni-plaintext\">[ 512, 128 ]</div></td><td>bert/embeddings/position_embeddings</td></tr><tr><td>3</td><td><div class=\"dni-plaintext\">[ 2, 128 ]</div></td><td>bert/embeddings/token_type_embeddings</td></tr><tr><td>4</td><td><div class=\"dni-plaintext\">[ 30522, 128 ]</div></td><td>bert/embeddings/word_embeddings</td></tr><tr><td>5</td><td><div class=\"dni-plaintext\">[ 128 ]</div></td><td>bert/encoder/layer_0/attention/output/LayerNorm/beta</td></tr><tr><td>6</td><td><div class=\"dni-plaintext\">[ 128 ]</div></td><td>bert/encoder/layer_0/attention/output/LayerNorm/gamma</td></tr><tr><td>7</td><td><div class=\"dni-plaintext\">[ 128 ]</div></td><td>bert/encoder/layer_0/attention/output/dense/bias</td></tr><tr><td>8</td><td><div class=\"dni-plaintext\">[ 128, 128 ]</div></td><td>bert/encoder/layer_0/attention/output/dense/kernel</td></tr><tr><td>9</td><td><div class=\"dni-plaintext\">[ 128 ]</div></td><td>bert/encoder/layer_0/attention/self/key/bias</td></tr><tr><td>10</td><td><div class=\"dni-plaintext\">[ 128, 128 ]</div></td><td>bert/encoder/layer_0/attention/self/key/kernel</td></tr><tr><td>11</td><td><div class=\"dni-plaintext\">[ 128 ]</div></td><td>bert/encoder/layer_0/attention/self/query/bias</td></tr><tr><td>12</td><td><div class=\"dni-plaintext\">[ 128, 128 ]</div></td><td>bert/encoder/layer_0/attention/self/query/kernel</td></tr><tr><td>13</td><td><div class=\"dni-plaintext\">[ 128 ]</div></td><td>bert/encoder/layer_0/attention/self/value/bias</td></tr><tr><td>14</td><td><div class=\"dni-plaintext\">[ 128, 128 ]</div></td><td>bert/encoder/layer_0/attention/self/value/kernel</td></tr><tr><td>15</td><td><div class=\"dni-plaintext\">[ 512 ]</div></td><td>bert/encoder/layer_0/intermediate/dense/bias</td></tr><tr><td>16</td><td><div class=\"dni-plaintext\">[ 128, 512 ]</div></td><td>bert/encoder/layer_0/intermediate/dense/kernel</td></tr><tr><td>17</td><td><div class=\"dni-plaintext\">[ 128 ]</div></td><td>bert/encoder/layer_0/output/LayerNorm/beta</td></tr><tr><td>18</td><td><div class=\"dni-plaintext\">[ 128 ]</div></td><td>bert/encoder/layer_0/output/LayerNorm/gamma</td></tr><tr><td>19</td><td><div class=\"dni-plaintext\">[ 128 ]</div></td><td>bert/encoder/layer_0/output/dense/bias</td></tr><tr><td colspan=\"3\">(24 more)</td></tr></tbody></table>"
          },
          "execution_count": 1,
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The Small BERT Model\n",
        "The small BERT model used here ('small_bert_bert_uncased_L-2_H-128') requries lower cased text and has hidden layer size of 128. It has two 'transformer' layers. There are many versions of BERT available - from tiny to large. See the [text classification tutorial](https://www.tensorflow.org/text/tutorials/classify_text_with_bert) from Tensforflow for more details.\n",
        "\n",
        "We will re-construct the BERT model using TorchSharp (.Net binding to PyTorch) code and then load the pre-trained weights. The weights will have to be mapped manually. This requires some knowledge of [Tranformers/BERT](https://arxiv.org/abs/1810.04805). However, our task is made easier because TorchSharp (PyTorch) provides a pre-built [TransformerEncoderLayer](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html) which encapulates the basic structure of Transformer-based models.\n",
        "\n",
        "We are required to reconstruct the exact structure of BERT layer-by-layer to ensure the weights are applicable. For re-training we may exclude the final layers and only build the model up to the output of the encoder. For our needs, we wil use the pre-trained weights that start with 'bert/' prefix (see above) and ignore the rest.\n",
        "\n",
        "### Bert Layers\n",
        "The top level layers we will need are:\n",
        "- *Embedding layer*: With sub-layers for word, position & token-type embeddings; layer normalization and dropout. Embedding maps an index to its corresponding learned feature vector.\n",
        "- *Transfomer Encoder layer*: With two sub-layers that apply the core transformer functionality.\n",
        "- *Pooling*: Pools (summarizes) the output sequence into a single value - its encoded representation\n",
        "\n",
        "The rest of the layers will be custom built for text classification (later).\n",
        "\n",
        "### Other Parameters\n",
        "Additional model parameters are required, e.g. vocabulary size, dropout rate, etc. Some of these maybe obtained from Hugging Face from the [BERT model 'card' config file](https://huggingface.co/bert-base-uncased/blob/main/config.json). Here we have defined the required parameters as constants in the code below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "dotnet_interactive": {
          "language": "fsharp"
        }
      },
      "source": [
        "//tensor dims - these values should match the relevant dimensions of the corresponding tensors in the checkpoint\n",
        "let HIDDEN      = 128L\n",
        "let VOCAB_SIZE  = 30522L    // see vocab.txt file included in the BERT download\n",
        "let TYPE_SIZE   = 2L         // bert needs 'type' of token\n",
        "let MAX_POS_EMB = 512L\n",
        "\n",
        "//other parameters\n",
        "let EPS_LAYER_NORM      = 1e-12\n",
        "let HIDDEN_DROPOUT_PROB = 0.1\n",
        "let N_HEADS             = 2L\n",
        "let ATTN_DROPOUT_PROB   = 0.1\n",
        "let ENCODER_LAYERS      = 2L\n",
        "let ENCODER_ACTIVATION  = torch.nn.Activations.GELU"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Embedding Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "dotnet_interactive": {
          "language": "fsharp"
        }
      },
      "source": [
        "//Note: The module and variable names used here match the tensor name 'paths' as delimted by '/' for TF (see above), \n",
        "//for easier matching.\n",
        "type BertEmbedding() as this = \n",
        "    inherit torch.nn.Module(\"embeddings\")\n",
        "    \n",
        "    let word_embeddings         = torch.nn.Embedding(VOCAB_SIZE,HIDDEN,padding_idx=0L)\n",
        "    let position_embeddings     = torch.nn.Embedding(MAX_POS_EMB,HIDDEN)\n",
        "    let token_type_embeddings   = torch.nn.Embedding(TYPE_SIZE,HIDDEN)\n",
        "    let LayerNorm               = torch.nn.LayerNorm([|HIDDEN|],EPS_LAYER_NORM)\n",
        "    let dropout                 = torch.nn.Dropout(HIDDEN_DROPOUT_PROB)\n",
        "\n",
        "    do \n",
        "        this.RegisterComponents()\n",
        "\n",
        "    member this.forward(input_ids:torch.Tensor, token_type_ids:torch.Tensor, position_ids:torch.Tensor) =   \n",
        "    \n",
        "        let embeddings =      \n",
        "            (input_ids       --> word_embeddings)        +\n",
        "            (token_type_ids  --> token_type_embeddings)  +\n",
        "            (position_ids    --> position_embeddings)\n",
        "\n",
        "        embeddings --> LayerNorm --> dropout             // the --> operator works for simple 'forward' invocations"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## BERT Pooler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "dotnet_interactive": {
          "language": "fsharp"
        }
      },
      "source": [
        "type BertPooler() as this = \n",
        "    inherit torch.nn.Module(\"pooler\")\n",
        "\n",
        "    let dense = torch.nn.Linear(HIDDEN,HIDDEN)\n",
        "    let activation = torch.nn.Tanh()\n",
        "\n",
        "    let ``:`` = torch.TensorIndex.Colon\n",
        "    let first = torch.TensorIndex.Single(0L)\n",
        "\n",
        "    do\n",
        "        this.RegisterComponents()\n",
        "\n",
        "    override _.forward (hidden_states) =\n",
        "        let first_token_tensor = hidden_states.index(``:``, first) //take first token of the sequence as the pooled value\n",
        "        first_token_tensor --> dense --> activation"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## BERT Model \n",
        "Combines the embedding, pooler and transformer encoder layers. (The transformer encoders are available out-of-the-box in PyTroch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "dotnet_interactive": {
          "language": "fsharp"
        }
      },
      "source": [
        "type BertModel() as this =\n",
        "    inherit torch.nn.Module(\"bert\")\n",
        "\n",
        "    let embeddings = new BertEmbedding()\n",
        "    let pooler = new BertPooler()\n",
        "\n",
        "    let encoderLayer = torch.nn.TransformerEncoderLayer(HIDDEN, N_HEADS, MAX_POS_EMB, ATTN_DROPOUT_PROB, activation=ENCODER_ACTIVATION)\n",
        "    let encoder = torch.nn.TransformerEncoder(encoderLayer, ENCODER_LAYERS)\n",
        "\n",
        "    do\n",
        "        this.RegisterComponents()\n",
        "    \n",
        "    member this.forward(input_ids:torch.Tensor, token_type_ids:torch.Tensor, position_ids:torch.Tensor,?mask:torch.Tensor) =\n",
        "        let src = embeddings.forward(input_ids, token_type_ids, position_ids)\n",
        "        let srcBatchDim2nd = src.permute(1L,0L,2L) //PyTorch transformer requires input as such. See the Transformer docs\n",
        "        let encoded = match mask with None -> encoder.forward(srcBatchDim2nd) | Some mask -> encoder.forward(srcBatchDim2nd,mask)\n",
        "        let encodedBatchFst = encoded.permute(1L,0L,2L)\n",
        "        encodedBatchFst --> pooler"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create a Test BERT Model Instance and Load Pre-Trained TF Weights\n",
        "The main task here is to find the right mapping between the parameters of the BertModel and those form the Tensorflow BERT checkpoint.\n",
        "\n",
        "There are several steps involved - first is create an empty model and list all the parameters in the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "dotnet_interactive": {
          "language": "fsharp"
        }
      },
      "source": [
        "let testBert = new BertModel()\n",
        "//bert.named_modules() \n",
        "testBert.named_parameters() |> Seq.map (fun struct(n,x) -> n,x.shape) |> Seq.iter (printfn \"%A\")"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.code.notebook.stdout": "(\"embeddings.word_embeddings.weight\", [|30522L; 128L|])\r\n(\"embeddings.position_embeddings.weight\", [|512L; 128L|])\r\n(\"embeddings.token_type_embeddings.weight\", [|2L; 128L|])\r\n(\"embeddings.LayerNorm.weight\", [|128L|])\r\n(\"embeddings.LayerNorm.bias\", [|128L|])\r\n(\"pooler.dense.weight\", [|128L; 128L|])\r\n(\"pooler.dense.bias\", [|128L|])\r\n(\"encoder.layers.0.self_attn.in_proj_weight\", [|384L; 128L|])\r\n(\"encoder.layers.0.self_attn.in_proj_bias\", [|384L|])\r\n(\"encoder.layers.0.self_attn.out_proj.weight\", [|128L; 128L|])\r\n(\"encoder.layers.0.self_attn.out_proj.bias\", [|128L|])\r\n(\"encoder.layers.0.linear1.weight\", [|512L; 128L|])\r\n(\"encoder.layers.0.linear1.bias\", [|512L|])\r\n(\"encoder.layers.0.linear2.weight\", [|128L; 512L|])\r\n(\"encoder.layers.0.linear2.bias\", [|128L|])\r\n(\"encoder.layers.0.norm1.weight\", [|128L|])\r\n(\"encoder.layers.0.norm1.bias\", [|128L|])\r\n(\"encoder.layers.0.norm2.weight\", [|128L|])\r\n(\"encoder.layers.0.norm2.bias\", [|128L|])\r\n(\"encoder.layers.1.self_attn.in_proj_weight\", [|384L; 128L|])\r\n(\"encoder.layers.1.self_attn.in_proj_bias\", [|384L|])\r\n(\"encoder.layers.1.self_attn.out_proj.weight\", [|128L; 128L|])\r\n(\"encoder.layers.1.self_attn.out_proj.bias\", [|128L|])\r\n(\"encoder.layers.1.linear1.weight\", [|512L; 128L|])\r\n(\"encoder.layers.1.linear1.bias\", [|512L|])\r\n(\"encoder.layers.1.linear2.weight\", [|128L; 512L|])\r\n(\"encoder.layers.1.linear2.bias\", [|128L|])\r\n(\"encoder.layers.1.norm1.weight\", [|128L|])\r\n(\"encoder.layers.1.norm1.bias\", [|128L|])\r\n(\"encoder.layers.1.norm2.weight\", [|128L|])\r\n(\"encoder.layers.1.norm2.bias\", [|128L|])\r\n"
          },
          "execution_count": 1,
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we compare the names above to the Tensorflow checkpoint names in the beginning, we can find clues as to how the two may be matched. However this is not straigtforward. We need to build some 'infrastructure' to make this work.\n",
        "\n",
        "### Tensor data access helpers\n",
        "First off are some utility functions to get and set data into PyTorch tensors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "dotnet_interactive": {
          "language": "fsharp"
        }
      },
      "source": [
        "module Tensor = \n",
        "    //Note: ensure 't matches tensor datatype otherwise ToArray might crash the app (i.e. exception cannot be caught)\n",
        "    let private _getData<'t when 't:>ValueType and 't:struct and 't : (new:unit->'t) > (t:torch.Tensor) =\n",
        "        let s = t.data<'t>()\n",
        "        s.ToArray()\n",
        "\n",
        "    let getData<'t when 't:>ValueType and 't:struct and 't : (new:unit->'t)>  (t:torch.Tensor) =\n",
        "        if t.device_type <> DeviceType.CPU then \n",
        "            //use t1 = t.clone()\n",
        "            use t2 = t.cpu()\n",
        "            _getData<'t> t2\n",
        "        else \n",
        "            _getData<'t> t\n",
        "  \n",
        "    let setData<'t when 't:>ValueType and 't:struct and 't : (new:unit->'t)> (t:torch.Tensor) (data:'t[]) =\n",
        "        if t.device_type = DeviceType.CPU |> not then failwith \"tensor has to be on cpu for setData\"        \n",
        "        let s = t.data<'t>()\n",
        "        s.CopyFrom(data,0,0L)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Name map\n",
        "The *nameMap* is a 3-tuple list: \n",
        "1. BertModel parameter name; \n",
        "2. List of TF tensor names that should be mapped to the parameter\n",
        "3. Post processing indicator. \n",
        "    \n",
        "In PyTorch, the encoder layer combines the query/key/value weights into a single parameter; these are separate in Tensorflow and therefore a list is requrired to map correctly.\n",
        "\n",
        "The post processing indicator (type **PostProc**) specifies the post processing required for each map entry.\n",
        "\n",
        "The *nameMap* list names contain wildcards ('#') which will be replaced by a number representing the encoder layer. BERT model versions can have different number of transformer layers. The model here has 2 layers but larger BERT models can have upto 12 layers. The wildcard-based mapping scheme is apt to handle an arbitrary number of layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "dotnet_interactive": {
          "language": "fsharp"
        }
      },
      "source": [
        "type PostProc = V | H | T | N\n",
        "\n",
        "let postProc (ts:torch.Tensor list) = function\n",
        "    | V -> torch.vstack(ResizeArray ts)\n",
        "    | H -> torch.hstack(ResizeArray ts)\n",
        "    | T -> ts.Head.T                  //Linear layer weights need to be transformed. See https://github.com/pytorch/pytorch/issues/2159\n",
        "    | N -> ts.Head\n",
        "\n",
        "let nameMap =\n",
        "    [\n",
        "        \"encoder.layers.#.self_attn.in_proj_weight\",[\"encoder/layer_#/attention/self/query/kernel\"; \n",
        "                                                     \"encoder/layer_#/attention/self/key/kernel\";    \n",
        "                                                     \"encoder/layer_#/attention/self/value/kernel\"],        V\n",
        "\n",
        "        \"encoder.layers.#.self_attn.in_proj_bias\",  [\"encoder/layer_#/attention/self/query/bias\";\n",
        "                                                     \"encoder/layer_#/attention/self/key/bias\"; \n",
        "                                                     \"encoder/layer_#/attention/self/value/bias\"],          H\n",
        "\n",
        "        \"encoder.layers.#.self_attn.out_proj.weight\", [\"encoder/layer_#/attention/output/dense/kernel\"],    N\n",
        "        \"encoder.layers.#.self_attn.out_proj.bias\",   [\"encoder/layer_#/attention/output/dense/bias\"],      N\n",
        "\n",
        "\n",
        "        \"encoder.layers.#.linear1.weight\",          [\"encoder/layer_#/intermediate/dense/kernel\"],          T\n",
        "        \"encoder.layers.#.linear1.bias\",            [\"encoder/layer_#/intermediate/dense/bias\"],            N\n",
        "\n",
        "        \"encoder.layers.#.linear2.weight\",          [\"encoder/layer_#/output/dense/kernel\"],                T\n",
        "        \"encoder.layers.#.linear2.bias\",            [\"encoder/layer_#/output/dense/bias\"],                  N\n",
        "\n",
        "        \"encoder.layers.#.norm1.weight\",            [\"encoder/layer_#/attention/output/LayerNorm/gamma\"],   N\n",
        "        \"encoder.layers.#.norm1.bias\",              [\"encoder/layer_#/attention/output/LayerNorm/beta\"],    N\n",
        "\n",
        "        \"encoder.layers.#.norm2.weight\",            [\"encoder/layer_#/output/LayerNorm/gamma\"],             N\n",
        "        \"encoder.layers.#.norm2.bias\",              [\"encoder/layer_#/output/LayerNorm/beta\"],              N\n",
        "\n",
        "        \"embeddings.word_embeddings.weight\"         , [\"embeddings/word_embeddings\"]           , N\n",
        "        \"embeddings.position_embeddings.weight\"     , [\"embeddings/position_embeddings\"]       , N\n",
        "        \"embeddings.token_type_embeddings.weight\"   , [\"embeddings/token_type_embeddings\"]     , N\n",
        "        \"embeddings.LayerNorm.weight\"               , [\"embeddings/LayerNorm/gamma\"]           , N\n",
        "        \"embeddings.LayerNorm.bias\"                 , [\"embeddings/LayerNorm/beta\"]            , N\n",
        "        \"pooler.dense.weight\"                       , [\"pooler/dense/kernel\"]                  , T\n",
        "        \"pooler.dense.bias\"                         , [\"pooler/dense/bias\"]                    , N\n",
        "    ]\n",
        "\n",
        "let PREFIX = \"bert\"\n",
        "let addPrefix (s:string) = $\"{PREFIX}/{s}\"\n",
        "let sub n (s:string) = s.Replace(\"#\",string n)\n",
        ""
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Name map helpers\n",
        "Functions to set the parameter values of a PyTorch module from a TF checkpoint and a 'nameMap'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "dotnet_interactive": {
          "language": "fsharp"
        }
      },
      "source": [
        "//create a PyTorch tensor from TF checkpoint tensor data\n",
        "let toFloat32Tensor (shpdTnsr:CheckpointReader.ShapedTensor) = \n",
        "    match shpdTnsr.Tensor with\n",
        "    | CheckpointReader.TensorData.TdFloat ds -> torch.tensor(ds, dimensions=shpdTnsr.Shape)\n",
        "    | _                                      -> failwith \"TdFloat expected\"\n",
        "\n",
        "//set the value of a single parameter\n",
        "let performMap (tfMap:Map<string,_>) (ptMap:Map<string,Modules.Parameter>) (torchName,tfNames,postProcType) = \n",
        "    let torchParm = ptMap.[torchName]\n",
        "    let fromTfWts = tfNames |> List.map (fun n -> tfMap.[n] |> toFloat32Tensor) \n",
        "    let parmTensor = postProc fromTfWts postProcType\n",
        "    if torchParm.shape <> parmTensor.shape then failwithf $\"Mismatched weights for parameter {torchName}; parm shape: %A{torchParm.shape} vs tensor shape: %A{parmTensor.shape}\"\n",
        "    Tensor.setData<float32> torchParm (Tensor.getData<float32>(parmTensor))\n",
        "\n",
        "//set the parameter weights of a PyTorch model given checkpoint and nameMap\n",
        "let loadWeights (model:torch.nn.Module) checkpoint encoderLayers nameMap =\n",
        "    let tfMap = checkpoint |> Map.ofSeq\n",
        "    let ptMap = model.named_parameters() |> Seq.map (fun struct(n,m) -> n,m) |> Map.ofSeq\n",
        "\n",
        "    //process encoder layers\n",
        "    for l in 0 .. encoderLayers - 1 do\n",
        "        nameMap\n",
        "        |> List.filter (fun (p:string,_,_) -> p.Contains(\"#\")) \n",
        "        |> List.map (fun (p,tns,postProc) -> sub l p, tns |> List.map (addPrefix >> (sub l)), postProc)\n",
        "        |> List.iter (performMap tfMap ptMap)\n",
        "\n",
        "    nameMap\n",
        "    |> List.filter (fun (p,_,_) -> p.Contains(\"#\") |> not)\n",
        "    |> List.map (fun (name,tns,postProcType) -> name, tns |> List.map addPrefix, postProcType)\n",
        "    |> List.iter (performMap tfMap ptMap)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load weights into test model instance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "dotnet_interactive": {
          "language": "fsharp"
        }
      },
      "source": [
        "loadWeights testBert tensors (int ENCODER_LAYERS) nameMap"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Quick check\n",
        "Do a quick check - print the value of one of the model parameters and compare that to the equivalent one from TF to see if the values look right."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "dotnet_interactive": {
          "language": "fsharp"
        }
      },
      "source": [
        "testBert.get_parameter(\"encoder.layers.0.self_attn.in_proj_weight\") |> Tensor.getData<float32>"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": "<table><thead><tr><th><i>index</i></th><th>value</th></tr></thead><tbody><tr><td>0</td><td><div class=\"dni-plaintext\">-0.05550384</div></td></tr><tr><td>1</td><td><div class=\"dni-plaintext\">0.0120992055</div></td></tr><tr><td>2</td><td><div class=\"dni-plaintext\">-0.008531141</div></td></tr><tr><td>3</td><td><div class=\"dni-plaintext\">-0.019300643</div></td></tr><tr><td>4</td><td><div class=\"dni-plaintext\">0.043453686</div></td></tr><tr><td>5</td><td><div class=\"dni-plaintext\">0.055428784</div></td></tr><tr><td>6</td><td><div class=\"dni-plaintext\">0.029344434</div></td></tr><tr><td>7</td><td><div class=\"dni-plaintext\">-0.10154877</div></td></tr><tr><td>8</td><td><div class=\"dni-plaintext\">-0.07228437</div></td></tr><tr><td>9</td><td><div class=\"dni-plaintext\">-0.00066868344</div></td></tr><tr><td>10</td><td><div class=\"dni-plaintext\">0.06803441</div></td></tr><tr><td>11</td><td><div class=\"dni-plaintext\">-0.0084804995</div></td></tr><tr><td>12</td><td><div class=\"dni-plaintext\">0.014778442</div></td></tr><tr><td>13</td><td><div class=\"dni-plaintext\">0.060766224</div></td></tr><tr><td>14</td><td><div class=\"dni-plaintext\">0.003934786</div></td></tr><tr><td>15</td><td><div class=\"dni-plaintext\">-0.00080541073</div></td></tr><tr><td>16</td><td><div class=\"dni-plaintext\">0.014359639</div></td></tr><tr><td>17</td><td><div class=\"dni-plaintext\">-0.066106506</div></td></tr><tr><td>18</td><td><div class=\"dni-plaintext\">-0.017999956</div></td></tr><tr><td>19</td><td><div class=\"dni-plaintext\">-0.05386257</div></td></tr><tr><td colspan=\"2\">(49132 more)</td></tr></tbody></table>"
          },
          "execution_count": 1,
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "dotnet_interactive": {
          "language": "fsharp"
        }
      },
      "source": [
        "tensors |> Seq.find (fun (n,_) -> n = \"bert/encoder/layer_0/attention/self/query/kernel\")"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": "<table><thead><tr><th>Item1</th><th>Item2</th></tr></thead><tbody><tr><td><div class=\"dni-plaintext\">bert/encoder/layer_0/attention/self/query/kernel</div></td><td><div class=\"dni-plaintext\">{ { Shape = [|128L; 128L|]\n  Tensor =\n   TdFloat\n     [|-0.05550384149f; 0.01209920552f; -0.008531141095f; -0.01930064335f;\n       0.04345368594f; 0.05542878434f; 0.02934443392f; -0.1015487686f;\n       -0.07228437066f; -0.0006686834386f; 0.06803441048f; -0.008480499499f;\n       0.01477844175f; 0.06076622382f; 0.003934786189f; -0.0008054107311f;\n       0.01435963903f; -0.06610650569f; -0.01799995638f; -0.05386257172f;\n       -0.03858022392f; -0.02942419797f; -0.01362462994f; 0.04548906162f;\n       0.02651916817f; 0.01372765377f; -0.04908275977f; -0.1125504076f;\n       0.007296630181f; 0.04891639203f; 0.04387928173f; 0.06503837556f;\n       -0.05659360439f; -0.006504856516f; 0.06188944727f; -0.1045558825f;\n       0.06272804737f; 0.1617943794f; -0.008180449717f; 0.005743560381f;\n       0.04920198023f; -0.02764579467f; -0.02393522486f; 0.07790721953f;\n       0.1218848452f; -0.1136314869f; 0.08718100935f; 0.03885361925f;\n       0.146388337f; 0.08435544372f; -0.01149796881f; 0.05596561357f;\n       -0.02651243284f; 0.08560265601f; -0.00856921915f; 0.120351024f;\n       -0.01969836466f; 0.2401848584f; 0.008963301778f; -0.05614889041f;\n       0.09748630226f; -0.03333476558f; 0.05127187818f; 0.03764935955f;\n       -0.07448612154f; 0.0264826864f; 0.01951963082f; -0.04107205197f;\n       -0.007767393254f; -0.008013598621f; 0.035505265f; -0.1104705185f;\n       0.05872561783f; 0.09439925104f; -0.02330717817f; -0.08990310878f;\n       -0.05722709373f; 0.06196752936f; 0.01164332032f; -0.009060089476f;\n       -0.01447457168f; 0.04950447381f; 0.007276773453f; -0.01481497008f;\n       0.1441659927f; 0.01024056226f; 0.01306775771f; 0.01946176961f;\n       -0.01554604061f; 0.01095542125f; -0.01752724685f; -0.1066188514f;\n       0.09672852606f; 0.05475880951f; -0.07761218399f; -0.04246133566f;\n       -0.05408534035f; 0.01053970307f; -0.1243368387f; -0.08822208643f; ...|] }: Shape: [ 128, 128 ], Tensor: { TdFloat\n  [|-0.05550384149f; 0.01209920552f; -0.008531141095f; -0.01930064335f;\n    0.04345368594f; 0.05542878434f; 0.02934443392f; -0.1015487686f;\n    -0.07228437066f; -0.0006686834386f; 0.06803441048f; -0.008480499499f;\n    0.01477844175f; 0.06076622382f; 0.003934786189f; -0.0008054107311f;\n    0.01435963903f; -0.06610650569f; -0.01799995638f; -0.05386257172f;\n    -0.03858022392f; -0.02942419797f; -0.01362462994f; 0.04548906162f;\n    0.02651916817f; 0.01372765377f; -0.04908275977f; -0.1125504076f;\n    0.007296630181f; 0.04891639203f; 0.04387928173f; 0.06503837556f;\n    -0.05659360439f; -0.006504856516f; 0.06188944727f; -0.1045558825f;\n    0.06272804737f; 0.1617943794f; -0.008180449717f; 0.005743560381f;\n    0.04920198023f; -0.02764579467f; -0.02393522486f; 0.07790721953f;\n    0.1218848452f; -0.1136314869f; 0.08718100935f; 0.03885361925f; 0.146388337f;\n    0.08435544372f; -0.01149796881f; 0.05596561357f; -0.02651243284f;\n    0.08560265601f; -0.00856921915f; 0.120351024f; -0.01969836466f;\n    0.2401848584f; 0.008963301778f; -0.05614889041f; 0.09748630226f;\n    -0.03333476558f; 0.05127187818f; 0.03764935955f; -0.07448612154f;\n    0.0264826864f; 0.01951963082f; -0.04107205197f; -0.007767393254f;\n    -0.008013598621f; 0.035505265f; -0.1104705185f; 0.05872561783f;\n    0.09439925104f; -0.02330717817f; -0.08990310878f; -0.05722709373f;\n    0.06196752936f; 0.01164332032f; -0.009060089476f; -0.01447457168f;\n    0.04950447381f; 0.007276773453f; -0.01481497008f; 0.1441659927f;\n    0.01024056226f; 0.01306775771f; 0.01946176961f; -0.01554604061f;\n    0.01095542125f; -0.01752724685f; -0.1066188514f; 0.09672852606f;\n    0.05475880951f; -0.07761218399f; -0.04246133566f; -0.05408534035f;\n    0.01053970307f; -0.1243368387f; -0.08822208643f; ...|]: Item: [ -0.05550384, 0.0120992055, -0.008531141, -0.019300643, 0.043453686, 0.055428784, 0.029344434, -0.10154877, -0.07228437, -0.00066868344, 0.06803441, -0.0084804995, 0.014778442, 0.060766224, 0.003934786, -0.00080541073, 0.014359639, -0.066106506, -0.017999956, -0.05386257 ... (16364 more) ] } }</div></td></tr></tbody></table>"
          },
          "execution_count": 1,
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Data\n",
        "The training dataset is the [Yelp review dataset](https://s3.amazonaws.com/fast-ai-nlp/yelp_review_polarity_csv.tgz). Assume this data is saved to a local folder as given below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "dotnet_interactive": {
          "language": "fsharp"
        }
      },
      "source": [
        "let foldr = @\"C:\\yelp_review_polarity_csv\"\n",
        "let testCsv = Path.Combine(foldr,\"test.csv\")\n",
        "let trainCsv = Path.Combine(foldr,\"train.csv\")\n",
        "if File.Exists testCsv |> not then failwith $\"File not found; path = {testCsv}\"\n",
        "printfn \"%A\" trainCsv"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.code.notebook.stdout": "\"C:\\yelp_review_polarity_csv\\train.csv\"\r\n"
          },
          "execution_count": 1,
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "dotnet_interactive": {
          "language": "fsharp"
        }
      },
      "source": [
        "open FSharp.Data\n",
        "type YelpCsv = FSharp.Data.CsvProvider< Sample=\"a,b\", HasHeaders=false, Schema=\"Label,Text\">\n",
        "type [<CLIMutable>] YelpReview = {Label:int; Text:string}\n",
        "//need to make labels 0-based so subtract 1\n",
        "let testSet = YelpCsv.Load(testCsv).Rows |> Seq.map (fun r-> {Label=int r.Label - 1; Text=r.Text}) |> Seq.toArray \n",
        "let trainSet = YelpCsv.Load(trainCsv).Rows |> Seq.map (fun r->{Label=int r.Label - 1; Text=r.Text}) |> Seq.toArray\n",
        "testSet.Display() "
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": "<table><thead><tr><th><i>index</i></th><th>Label</th><th>Text</th></tr></thead><tbody><tr><td>0</td><td><div class=\"dni-plaintext\">1</div></td><td>Contrary to other reviews, I have zero complaints about the service or the prices. I have been getting tire service here for the past 5 years now, and compared to my experience with places like Pep Boys, these guys are experienced and know what they&#39;re doing. \\nAlso, this is one place that I do not feel like I am being taken advantage of, just because of my gender. Other auto mechanics have been notorious for capitalizing on my ignorance of cars, and have sucked my bank account dry. But here, my service and road coverage has all been well explained - and let up to me to decide. \\nAnd they just renovated the waiting room. It looks a lot better than it did in previous years.</td></tr><tr><td>1</td><td><div class=\"dni-plaintext\">0</div></td><td>Last summer I had an appointment to get new tires and had to wait a super long time. I also went in this week for them to fix a minor problem with a tire they put on. They \\&quot;fixed\\&quot; it for free, and the very next morning I had the same issue. I called to complain, and the \\&quot;manager\\&quot; didn&#39;t even apologize!!! So frustrated. Never going back.  They seem overpriced, too.</td></tr><tr><td>2</td><td><div class=\"dni-plaintext\">1</div></td><td>Friendly staff, same starbucks fair you get anywhere else.  Sometimes the lines can get long.</td></tr><tr><td>3</td><td><div class=\"dni-plaintext\">0</div></td><td>The food is good. Unfortunately the service is very hit or miss. The main issue seems to be with the kitchen, the waiters and waitresses are often very apologetic for the long waits and it&#39;s pretty obvious that some of them avoid the tables after taking the initial order to avoid hearing complaints.</td></tr><tr><td>4</td><td><div class=\"dni-plaintext\">1</div></td><td>Even when we didn&#39;t have a car Filene&#39;s Basement was worth the bus trip to the Waterfront. I always find something (usually I find 3-4 things and spend about $60) and better still, I am always still wearing the clothes and shoes 3 months later. \\n\\nI kind of suspect this is the best shopping in Pittsburgh; it&#39;s much better than the usual department stores, better than Marshall&#39;s and TJ Maxx and better than the Saks downtown, even when it has a sale. Selection, bargains AND quality.\\n\\nI like this Filene&#39;s better than Gabriel Brothers, which are harder to get to. Gabriel Brothers are a real discount shopper&#39;s challenge and I&#39;m afraid I didn&#39;t live in Pittsburgh long enough to develop the necessary skills . . . Filene&#39;s was still up and running in June 2007 when I left town.</td></tr><tr><td>5</td><td><div class=\"dni-plaintext\">1</div></td><td>Picture Billy Joel&#39;s \\&quot;Piano Man\\&quot; DOUBLED mixed with beer, a rowdy crowd, and comedy - Welcome to Sing Sing!  A unique musical experience found in Homestead.\\n\\nIf you&#39;re looking to grab a bite to eat or a beer, come on in!  Serving food and brews from Rock Bottom Brewery, Sing Sing keeps your tummy full while you listen to two (or more) amazingly talented pianists take your musical requests.  They&#39;ll play anything you&#39;d like, for tips of course.  Wanting to hear Britney Spears?  Toto?  Duran Duran?  Yep, they play that... new or old.\\n\\nThe crowd makes the show, so make sure you come ready for a good time.  If the crowd is dead, it&#39;s harder for the Guys to get a reaction.  If you&#39;re wanting to have some fun, it can be a GREAT time!  It&#39;s the perfect place for Birthday parties - especially if you want to embarrass a friend.  The guys will bring them up to the pianos and perform a little ditty.  For being a good sport, you get the coveted Sing Sing bumper sticker.  Now who wouldn&#39;t want that?\\n\\nDueling Pianos and brews... time to Shut Up &amp; Sing Sing!</td></tr><tr><td>6</td><td><div class=\"dni-plaintext\">0</div></td><td>Mediocre service. COLD food! Our food waited so long the lettuce &amp; pickles wilted. Bland food. Crazy overpriced. Long waits in the arcade. 1 beer per hour maximum.  Avoid at all costs. Fair manager.</td></tr><tr><td>7</td><td><div class=\"dni-plaintext\">0</div></td><td>Ok! Let me tell you about my bad experience first. I went to D&amp;B last night for a post wedding party - which, side note, is a great idea!\\n\\nIt was around midnight and the bar wasn&#39;t really populated. There were three bartenders and only one was actually making rounds to see if anyone needed anything. The two other bartenders were chatting on the far side of the bar that no one was sitting at. Kind of counter productive if you ask me. \\n\\nI stood there for about 5 minutes, which for a busy bar is fine but when I am the only one with my card out then, it just seems a little ridiculous. I made eye contact with the one girl twice and gave her a smile and she literally turned away. I finally had to walk to them to get their attention.  I was standing right in front of them smiling and they didn&#39;t ask if i need anything. I finally said, \\&quot;Are you working?\\&quot; and they gave each other a weird look. I felt like i was the crazy one. I asked for a beer/got the beer.\\n\\nIn between that time, the other bartender brought food over and set it down. She took a fry from the plate (right in front of me) and then served it to someone on the other side of the bar. What the hell! I felt like i was in some grimy bar in out in the sticks - not an established D&amp;B. \\n\\nI was just really turned off from that experience. \\n\\nThe good is that D&amp;B provides a different type of entertainment when you want to mix things up. I remember going here with my grandparents when I was a kid and it was the best treat ever! We would eat at the restaurant and then spend hours playing games. This place holds some really good memories for me. \\n\\nIt&#39;s a shame that my experience last night has spoiled the high standards I held for it.</td></tr><tr><td>8</td><td><div class=\"dni-plaintext\">0</div></td><td>I used to love D&amp;B when it first opened in the Waterfront, but it has gone down hill over the years. The games are not as fun and do not give you as many tickets and the prizes have gotten cheaper in quality. It takes a whole heck of a lot of tickets for you to even get a pencil! The atmosphere is okay but it used to be so much better with the funnest games and diverse groups of people! Now, it is run down and many of the games are app related games (Fruit Ninja) and 3D Experience rides. With such \\&quot;games\\&quot;, you can&#39;t even earn tickets and they take a lot of tokens! Last time I went, back in the winter, many of the games were broken, which made for a negative player experience. I would go to D&amp;B to play some games again in the future, but it is no longer one of my favorite places to go due to the decline of fun games where you can earn tickets.</td></tr><tr><td>9</td><td><div class=\"dni-plaintext\">1</div></td><td>Like any Barnes &amp; Noble, it has a nice comfy cafe, and a large selection of books.  The staff is very friendly and helpful.  They stock a decent selection, and the prices are pretty reasonable.  Obviously it&#39;s hard for them to compete with Amazon.  However since all the small shop bookstores are gone, it&#39;s nice to walk into one every once in a while.</td></tr><tr><td>10</td><td><div class=\"dni-plaintext\">0</div></td><td>Meh, I&#39;ve experienced better is an understatement.\\n\\nFriday&#39;s is like the \\&quot;fine dining chain\\&quot; for every yinzer in Pittsburgh...\\n\\n1.  When we were seated, it was quick which was a surprise.  Somehow this restaurant gets packed and I never understand why.  I was happy about that.\\n2.  I ordered one of their \\&quot;skinny margaritas\\&quot;, blackberry to be exact.  It was a nice size, however, at $6.00/pop, that was half of my food bill.\\n3.  The waitress started off attentive, but after our food came out she was gone.  I ordered a turkey burger with pickle and mustard.  Loaded mashed potato on the side because I wanted heartburn ha!\\n4.  Food came.  My burger had lettuce on it only.  Waitress was supposed to go and get our next round of drinks.  I had to wait to ask for my pickle.\\n5.  The loaded potatoes were more like rubber than potatoes and not what I was expecting.  Disappointment.\\n6.  The waitress then went into oblivion and only returned with our check.  \\n\\nNever again will I eat at Friday&#39;s.  I will, however, go should a happy hour arise...</td></tr><tr><td>11</td><td><div class=\"dni-plaintext\">1</div></td><td>Unos has been around for ever, &amp; I feel like this restaurant chain peak in popularity in the 80&#39;s. Honestly the decor inside still kind of looks 80s to me even though its nice with sleek booth and exposed brick.\\n\\nIf you haven&#39;t died and he recently I ordered you to come back and have a meal here again because honestly the food is really quite good!\\n\\nThey have the best chicken salad wrap ever! I love that rap so much I want agreed to walk the south side River Trail from where the Steelers practice all the way to the damn waterfront just because I knew that I could convince my boyfriend to go to Unos with me for lunch.  Full disclosure:  I made him call is a cab and we took a taxi back to the parking lot after lunch.\\n\\nListen... The food and pizza and service are very good, surprisingly so! I don&#39;t know why this place is not busier but next time you&#39;re down at the Waterfront please do consider dining here!</td></tr><tr><td>12</td><td><div class=\"dni-plaintext\">0</div></td><td>Stars are for the food only.    2.5 rating\\n\\nGot there at 8pm on Saturday night, they told us it was an hour wait which was expected, but it only took 30minutes to be seated so that was a bonus.  Started off with the lettuce wraps and i used to be a fan of these but they just seem to bore me now(FYI-love the lettuce wraps at the cheesecake factory).  I had the Singapore rice noodles.  This was so so as i thought immediately that lu lu noodles and asian cafe are better.  The fried rice was disappointing and the vegetables tasted raw.   Some good notes were the Dynamite shrimp, plump, juicy, tangy with a small kick.    Sweet and sour chicken was just the same minus the kick. \\n\\nThe service was annoying.   We were there less than 45 minutes and it seemed like our waiter was in a rush to get us out of there.   Constantly asking if he could remove my plate.  I had to sternly say at one point that I will let you know when i am finished.  I found it rude especially since  there wasn&#39;t even a line at this point.  \\n\\nI think we will travel elsewhere following a movie.  The result never quite met the anticipation.</td></tr><tr><td>13</td><td><div class=\"dni-plaintext\">0</div></td><td>Our last few visits have left something to desire, One of the things I always notice is the demeanor of an establishments employees. These people look miserable, no one smiles or greets you appropriately, it&#39;s more like get in, get out, turn the tables,\\n\\nThe food has remained consistently good... when we can order it. the last time JB and I came for a quick dinner, the place was not busy yet we were passed by 6 frowning waitstaff without getting drinks... for 15 minutes we were left suiting at our table with our menus closed and sitting on the edge of the table (HINT, HINT) so we left and most likely will not be returning.</td></tr><tr><td>14</td><td><div class=\"dni-plaintext\">1</div></td><td>Good quality pork fried dumpling and mongo beef.  Bar service was fast and Efficent.   Good value.  Not a 5 cause dragon eye tea was cold first time around and beef had minor amount of fat.   Fortune was even Postive!</td></tr><tr><td>15</td><td><div class=\"dni-plaintext\">0</div></td><td>Very bad purchase experience. I bought a shirt with a hole covered in the rolled up sleeves, but they denied my request to return it. I am so angery at this and will never shop their chothes anymore.</td></tr><tr><td>16</td><td><div class=\"dni-plaintext\">0</div></td><td>When I think BBB... I think the days of simply bringing your bike in for a quick and relatively inexpensive tune-up and a few fixes are long gone.    \\n\\nThis review is more for the repair end of BBB. In their defense BBB does appear to carry some amazing brands of bike (ie Colnago) that you just don&#39;t find anywhere else in Pittsburgh. \\n\\nAt BBB I was charged $250 for a tune up and a few other things. Granted this included installing a sew up tire (which I can understand would cost approx $50), Swapping out a left side (big ring) shifter on my down tube (this should have cost approx. $20 at most) and installing new bar tape (cost of tape $20 and $20 to install?).. SO WHAT\\&quot;S WITH $140 FOR A TUNE UP? Well the story goes like this:\\n\\nI bring the bike into BBB prior to the nice weather hitting Pittsburgh in hopes of trying what people have said is a great bike shop and getting my OCLV TREK 5900 ready for the season. Turns out I don&#39;t hear from these guys. A week goes by ...two weeks...I think that&#39;s ok I have two or three other bike I can turn to for a ride. Then I wind up going out of town for a week thinking for sure I&#39;ll get a call from them re: my bike is ready to roll...but no dice. So I call. Turns out a screw snapped when the mechanic was re-installing the down tube shifter and it had to be tapped out (is that my fault?). He says \\&quot;Should be ready in a few days\\&quot;. So I come in a few days later to this mammoth bill. I ask if I am paying for the labor of taping out the screw? I don&#39;t think I ever got a straight answer? I look at the bill and can&#39;t see a good breakdown of the charges. Normally I would \\&quot;duke it over\\&quot; a bill like this but I figured...I had somewhere I to be 10 minutes ago and at least I finally have my bike. I would expect that for that money my bike could have been stripped down to the frame and totally gone over (overhauled). But it wasn&#39;t.  Well BBB I&#39;ll give you a star because the mechanic did do a good job in that my cycle shifts well and the tape job on the bars looks great (nice wrap). Plus I&#39;ll toss in a star for your outstanding selection of high end cycles. Maybe I would have rated BBB higher if I was in the market for a purchase instead of a simple repair?</td></tr><tr><td>17</td><td><div class=\"dni-plaintext\">0</div></td><td>My orders have come out bad pretty much every time I have ordered from here. The chicken nuggets come out under par. I&#39;m lucky if the fries come out right. My last experience is the last straw. I recently posted a photo of my Chicken Ranch Club. That has no chicken. This is ridiculous. I am done going to this Mcd&#39;s.</td></tr><tr><td>18</td><td><div class=\"dni-plaintext\">0</div></td><td>Wow. What a shame. My family just moved to the area and have been sampling the local cuisine. We are a very large family and love foods of all kind. We&#39;ve eaten Chinese food from a hundred different places in a dozen different cities. Hands down, this was the worst we&#39;ve ever had. Cold, tough, congealed, and tasteless - that&#39;s the only way to describe it. I&#39;ll never order from here again. Quite frankly, the fact that I took time away from my kids to type this out should tell you how awful it was. My complaints fell on deaf ears. I guess they were shocked that someone thought that they were serving low grade dog food. I understand that sometimes restaurants crank out bad food. It happens. But when it does, you bend over backwards to make it right. Not at this place. I was told that he would have to speak with the owner in order to help me. I guess that&#39;s how they deflect complaints - blame it on the owners that aren&#39;t there and make the customer for away angry.  Too bad. Because we will be visiting a competitor in the future. This family of 7 won&#39;t be visiting China Sea ever again. Oh, and by the way, why can&#39;t I give this place zero stars?</td></tr><tr><td>19</td><td><div class=\"dni-plaintext\">1</div></td><td>I fell in love with this place as soon as we pulled up and saw the lights strung up and  oldies coming from the speakers! \\n\\nI tried the banana cream pie hard ice cream, their scoops are very generous!! \\n\\nMy bf got the peach cobbler hard ice cream and that was to die for! We got 4 servings of ice cream for $10, which nowadays is a steal IMO! :) \\n\\nI&#39;ll definitely be heading back with my coworkers this week!</td></tr><tr><td colspan=\"3\">(37980 more)</td></tr></tbody></table>"
          },
          "execution_count": 1,
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Calculate the number of label classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "dotnet_interactive": {
          "language": "fsharp"
        }
      },
      "source": [
        "let classes = trainSet |> Seq.map (fun x->x.Label) |> set\n",
        "classes.Display()\n",
        "let TGT_LEN = classes.Count |> int64"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": "<table><thead><tr><th><i>index</i></th><th>value</th></tr></thead><tbody><tr><td>0</td><td><div class=\"dni-plaintext\">0</div></td></tr><tr><td>1</td><td><div class=\"dni-plaintext\">1</div></td></tr></tbody></table>"
          },
          "execution_count": 1,
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Batch processing\n",
        "Helpers for serving minibatches of tensors for training and evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "dotnet_interactive": {
          "language": "fsharp"
        }
      },
      "source": [
        "let BATCH_SIZE = 128\n",
        "let trainBatches = trainSet |> Seq.chunkBySize BATCH_SIZE\n",
        "let testBatches  = testSet  |> Seq.chunkBySize BATCH_SIZE\n",
        "open BERTTokenizer\n",
        "let vocabFile = @\"C:\\s\\hack\\small_bert_bert_uncased_L-2_H-128_A-2_2\\assets\\vocab.txt\"\n",
        "let vocab = Vocabulary.loadFromFile vocabFile\n",
        "\n",
        "let position_ids = torch.arange(MAX_POS_EMB).expand(int64 BATCH_SIZE,-1L).``to``(device)\n",
        "\n",
        "//convert a batch to input and output (X, Y) tensors\n",
        "let toXY (batch:YelpReview[]) = \n",
        "    let xs = batch |> Array.map (fun x-> Featurizer.toFeatures vocab true (int MAX_POS_EMB) x.Text \"\")\n",
        "    let d_tkns      = xs |> Seq.collect (fun f -> f.InputIds )  |> Seq.toArray\n",
        "    let d_tkn_typs  = xs |> Seq.collect (fun f -> f.SegmentIds) |> Seq.toArray\n",
        "    let tokenIds = torch.tensor(d_tkns,     dtype=torch.int).view(-1L,MAX_POS_EMB)        \n",
        "    let sepIds   = torch.tensor(d_tkn_typs, dtype=torch.int).view(-1L,MAX_POS_EMB)\n",
        "    let Y = torch.tensor(batch |> Array.map (fun x->x.Label), dtype=torch.int64).view(-1L)\n",
        "    (tokenIds,sepIds),Y"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Quick model check\n",
        "Evaluate bert instance with just the first batch of the training data to ensure its can produce the expected output.\n",
        "The expected output is a tensor with the shape BATCH_SIZE x HIDDEN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "dotnet_interactive": {
          "language": "fsharp"
        }
      },
      "source": [
        "testBert.Eval()\n",
        "let (_tkns,_seps),_ = trainBatches |> Seq.head |> toXY\n",
        "//_tkns.shape\n",
        "//_tkns |> Tensor.getData<int64>\n",
        "let _testOut = testBert.forward(_tkns,_seps,position_ids.cpu()) //test is on cpu\n",
        "_testOut.shape.Display()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": "<table><thead><tr><th><i>index</i></th><th>value</th></tr></thead><tbody><tr><td>0</td><td><div class=\"dni-plaintext\">128</div></td></tr><tr><td>1</td><td><div class=\"dni-plaintext\">128</div></td></tr></tbody></table>"
          },
          "execution_count": 1,
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Extend for classification\n",
        "Here the PyTorch multi-class classification method is used. The number of classes is only two for this data but the multi-class method is more general and can be easily extended to more than two classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "dotnet_interactive": {
          "language": "fsharp"
        }
      },
      "source": [
        "type BertClassification() as this = \n",
        "    inherit torch.nn.Module(\"BertClassification\")\n",
        "\n",
        "    let bert = new BertModel()\n",
        "    let proj = torch.nn.Linear(HIDDEN,TGT_LEN)\n",
        "\n",
        "    do\n",
        "        this.RegisterComponents()\n",
        "        this.LoadBertPretrained()\n",
        "\n",
        "    member _.LoadBertPretrained() =\n",
        "        loadWeights bert tensors (int ENCODER_LAYERS) nameMap\n",
        "    \n",
        "    member _.forward(tknIds,sepIds,pstnIds) =\n",
        "        use encoded = bert.forward(tknIds,sepIds,pstnIds)\n",
        "        encoded --> proj "
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training and evaluation code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "dotnet_interactive": {
          "language": "fsharp"
        }
      },
      "source": [
        "let _model = new BertClassification()\n",
        "_model.``to``(device)\n",
        "let _loss = torch.nn.functional.cross_entropy_loss()\n",
        "let mutable EPOCHS = 1\n",
        "let mutable verbose = true\n",
        "let gradCap = 0.1f\n",
        "let gradMin,gradMax = (-gradCap).ToScalar(),  gradCap.ToScalar()\n",
        "let opt = torch.optim.Adam(_model.parameters (), 0.001, amsgrad=true)       \n",
        "\n",
        "let class_accuracy (y:torch.Tensor) (y':torch.Tensor) =\n",
        "    use i = y'.argmax(1L)\n",
        "    let i_t = Tensor.getData<int64>(i)\n",
        "    let m_t = Tensor.getData<int64>(y)\n",
        "    Seq.zip i_t m_t \n",
        "    |> Seq.map (fun (a,b) -> if a = b then 1.0 else 0.0) \n",
        "    |> Seq.average\n",
        "\n",
        "//adjustment for end of data when full batch may not be available\n",
        "let adjPositions currBatchSize = if int currBatchSize = BATCH_SIZE then position_ids else torch.arange(MAX_POS_EMB).expand(currBatchSize,-1L).``to``(device)\n",
        "\n",
        "let dispose ls = ls |> List.iter (fun (x:IDisposable) -> x.Dispose())\n",
        "\n",
        "//run a batch through the model; return true output, predicted output and loss tensors\n",
        "let processBatch ((tkns:torch.Tensor,typs:torch.Tensor), y:torch.Tensor) =\n",
        "    use tkns_d = tkns.``to``(device)\n",
        "    use typs_d = typs.``to``(device)\n",
        "    let y_d    = y.``to``(device)            \n",
        "    let pstns  = adjPositions tkns.shape.[0]\n",
        "    if device <> torch.CPU then //these were copied so ok to dispose old tensors\n",
        "        dispose [tkns; typs; y]\n",
        "    let y' = _model.forward(tkns_d,typs_d,pstns)\n",
        "    let loss = _loss.Invoke(y', y_d)   \n",
        "    y_d,y',loss\n",
        "\n",
        "//evaluate on test set; return cross-entropy loss and classification accuracy\n",
        "let evaluate e =\n",
        "    _model.Eval()\n",
        "    let lss =\n",
        "        testBatches \n",
        "        |> Seq.map toXY\n",
        "        |> Seq.map (fun batch ->\n",
        "            let y,y',loss = processBatch batch\n",
        "            let ls = loss.ToDouble()\n",
        "            let acc = class_accuracy y y'            \n",
        "            dispose [y;y';loss]\n",
        "            GC.Collect()\n",
        "            ls,acc)\n",
        "        |> Seq.toArray\n",
        "    let ls  = lss |> Seq.averageBy fst\n",
        "    let acc = lss |> Seq.averageBy snd\n",
        "    ls,acc\n",
        "\n",
        "let mutable e = 0\n",
        "let train () =\n",
        "    \n",
        "    while e < EPOCHS do\n",
        "        e <- e + 1\n",
        "        _model.Train()\n",
        "        let losses = \n",
        "            trainBatches \n",
        "            |> Seq.map toXY\n",
        "            |> Seq.mapi (fun i batch ->                 \n",
        "                opt.zero_grad ()   \n",
        "                let y,y',loss = processBatch batch\n",
        "                let ls = loss.ToDouble()  \n",
        "                loss.backward()\n",
        "                _model.parameters() |> Array.iter (fun t -> t.grad().clip(gradMin,gradMax) |> ignore)                            \n",
        "                use  t_opt = opt.step ()\n",
        "                if verbose && i % 100 = 0 then\n",
        "                    let acc = class_accuracy y y'\n",
        "                    printfn $\"Epoch: {e}, minibatch: {i}, ce: {ls}, accuracy: {acc}\"                            \n",
        "                dispose [y;y';loss]\n",
        "                GC.Collect()\n",
        "                ls)\n",
        "            |> Seq.toArray\n",
        "\n",
        "        let evalCE,evalAcc = evaluate e\n",
        "        printfn $\"Epoch {e} train: {Seq.average losses}; eval acc: {evalAcc}\"\n",
        "\n",
        "    printfn \"Done train\"\n",
        "\n",
        "let runner () = async { do train () } "
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "dotnet_interactive": {
          "language": "fsharp"
        }
      },
      "source": [
        "runner() |> Async.RunSynchronously\n",
        "(*\n",
        "\n",
        "sample output:\n",
        "...\n",
        "Epoch: 2, minibatch: 4200, ce: 0.14490307867527008, accuracy: 0.9375\n",
        "Epoch: 2, minibatch: 4300, ce: 0.04636668041348457, accuracy: 0.984375\n",
        "Epoch 2 train: 0.15354100534277304; eval acc: 0.9376728595478595\n",
        "*)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.code.notebook.stdout": "Epoch: 1, minibatch: 0, ce: 0.6396772265434265, accuracy: 0.6640625\r\nEpoch: 1, minibatch: 100, ce: 0.35450270771980286, accuracy: 0.828125\r\nEpoch: 1, minibatch: 200, ce: 0.27361512184143066, accuracy: 0.8828125\r\nEpoch: 1, minibatch: 300, ce: 0.3631458580493927, accuracy: 0.8515625\r\nEpoch: 1, minibatch: 400, ce: 0.1945457011461258, accuracy: 0.9296875\r\nEpoch: 1, minibatch: 500, ce: 0.18711347877979279, accuracy: 0.9375\r\nEpoch: 1, minibatch: 600, ce: 0.187125563621521, accuracy: 0.921875\r\nEpoch: 1, minibatch: 700, ce: 0.20254415273666382, accuracy: 0.9453125\r\nEpoch: 1, minibatch: 800, ce: 0.25086578726768494, accuracy: 0.8828125\r\nEpoch: 1, minibatch: 900, ce: 0.19532586634159088, accuracy: 0.9296875\r\nEpoch: 1, minibatch: 1000, ce: 0.2735922932624817, accuracy: 0.875\r\nEpoch: 1, minibatch: 1100, ce: 0.20259249210357666, accuracy: 0.9140625\r\nEpoch: 1, minibatch: 1200, ce: 0.27378416061401367, accuracy: 0.8671875\r\nEpoch: 1, minibatch: 1300, ce: 0.2182709127664566, accuracy: 0.921875\r\nEpoch: 1, minibatch: 1400, ce: 0.21342137455940247, accuracy: 0.921875\r\nEpoch: 1, minibatch: 1500, ce: 0.11232473701238632, accuracy: 0.9375\r\nEpoch: 1, minibatch: 1600, ce: 0.13855941593647003, accuracy: 0.9375\r\nEpoch: 1, minibatch: 1700, ce: 0.16214805841445923, accuracy: 0.9453125\r\nEpoch: 1, minibatch: 1800, ce: 0.12941423058509827, accuracy: 0.9609375\r\nEpoch: 1, minibatch: 1900, ce: 0.2680433988571167, accuracy: 0.8984375\r\nEpoch: 1, minibatch: 2000, ce: 0.2538900077342987, accuracy: 0.875\r\nEpoch: 1, minibatch: 2100, ce: 0.23153211176395416, accuracy: 0.9375\r\nEpoch: 1, minibatch: 2200, ce: 0.22918573021888733, accuracy: 0.8984375\r\nEpoch: 1, minibatch: 2300, ce: 0.14171627163887024, accuracy: 0.9375\r\nEpoch: 1, minibatch: 2400, ce: 0.19525408744812012, accuracy: 0.890625\r\nEpoch: 1, minibatch: 2500, ce: 0.1602921336889267, accuracy: 0.953125\r\nEpoch: 1, minibatch: 2600, ce: 0.20208609104156494, accuracy: 0.9140625\r\nEpoch: 1, minibatch: 2700, ce: 0.22093532979488373, accuracy: 0.8984375\r\nEpoch: 1, minibatch: 2800, ce: 0.2366902232170105, accuracy: 0.875\r\nEpoch: 1, minibatch: 2900, ce: 0.15383455157279968, accuracy: 0.953125\r\nEpoch: 1, minibatch: 3000, ce: 0.15874269604682922, accuracy: 0.9296875\r\nEpoch: 1, minibatch: 3100, ce: 0.07738253474235535, accuracy: 0.9765625\r\nEpoch: 1, minibatch: 3200, ce: 0.17000482976436615, accuracy: 0.9453125\r\nEpoch: 1, minibatch: 3300, ce: 0.18760527670383453, accuracy: 0.921875\r\nEpoch: 1, minibatch: 3400, ce: 0.07021185755729675, accuracy: 0.9921875\r\nEpoch: 1, minibatch: 3500, ce: 0.09825675934553146, accuracy: 0.96875\r\nEpoch: 1, minibatch: 3600, ce: 0.21833346784114838, accuracy: 0.9296875\r\nEpoch: 1, minibatch: 3700, ce: 0.1617439091205597, accuracy: 0.9453125\r\nEpoch: 1, minibatch: 3800, ce: 0.13888947665691376, accuracy: 0.9453125\r\nEpoch: 1, minibatch: 3900, ce: 0.12057818472385406, accuracy: 0.9765625\r\nEpoch: 1, minibatch: 4000, ce: 0.17742185294628143, accuracy: 0.921875\r\nEpoch: 1, minibatch: 4100, ce: 0.0958794504404068, accuracy: 0.953125\r\nEpoch: 1, minibatch: 4200, ce: 0.13450421392917633, accuracy: 0.953125\r\nEpoch: 1, minibatch: 4300, ce: 0.06206202507019043, accuracy: 0.9921875\r\nEpoch 1 train: 0.20662290578910283; eval acc: 0.9361772486772486\r\nDone train\r\n"
          },
          "execution_count": 1,
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".NET (C#)",
      "language": "C#",
      "name": ".net-csharp"
    },
    "language_info": {
      "file_extension": ".cs",
      "mimetype": "text/x-csharp",
      "name": "C#",
      "pygments_lexer": "csharp",
      "version": "8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}